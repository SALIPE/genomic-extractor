{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/rrm-genomic-extractor`\n"
     ]
    },
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: `remove` not defined in `Pkg`\nSuggestion: check for spelling errors or missing imports.",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `remove` not defined in `Pkg`\n",
      "Suggestion: check for spelling errors or missing imports.\n",
      "\n",
      "Stacktrace:\n",
      " [1] getproperty(x::Module, f::Symbol)\n",
      "   @ Base ./Base.jl:42\n",
      " [2] top-level scope\n",
      "   @ ~/Desktop/rrm-genomic-extractor/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W1sZmlsZQ==.jl:4"
     ]
    }
   ],
   "source": [
    "begin\n",
    "    using Pkg\n",
    "    Pkg.activate(\".\")\n",
    "    Pkg.instantiate()\n",
    "\n",
    "end\n",
    "\n",
    "include(\"modules/DataIO.jl\")\n",
    "include(\"modules/Model.jl\")\n",
    "include(\"modules/EntropyUtil.jl\")\n",
    "\n",
    "using FLoops,\n",
    "    FASTX,\n",
    "    BioSequences,\n",
    "    LinearAlgebra,\n",
    "    Normalization,\n",
    "    Statistics,\n",
    "    Plots,\n",
    "    AbstractFFTs,\n",
    "    Flux,\n",
    "    Optimisers,\n",
    "    DecisionTree,\n",
    "    ScikitLearn\n",
    "\n",
    "using\n",
    "    .DataIO,\n",
    "    .Model,\n",
    "    .EntropyUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Tuple{BitVector, Vector{Tuple{Int64, Int64, Vector{Float64}}}, Vector{String}}} with 5 entries:\n",
       "  \"Omicron\" => ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]…\n",
       "  \"Beta\"    => ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]…\n",
       "  \"Gamma\"   => ([1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  1, 1, 1, 1, 1, 1, 1, 1, 1, 1]…\n",
       "  \"Delta\"   => ([1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]…\n",
       "  \"Alpha\"   => ([0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0]…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fasta::String = \"/home/salipe/Desktop/datasets/test_voc/test/Alpha.fasta\"\n",
    "modelCachedFile::String = \"$(pwd())/.project_cache/trained_model.dat\"\n",
    "\n",
    "sequences = Vector{String}()\n",
    "for record in open(FASTAReader, fasta)\n",
    "    seq::String = sequence(String, record)\n",
    "    # id::String = identifier(record)\n",
    "    # push!(sequences, (replace(id, r\"\\/|\\|\" => \"_\"), codeunits(seq)))\n",
    "    push!(sequences, seq)\n",
    "end\n",
    "\n",
    "model::Union{Nothing,Dict{String,Tuple{BitArray,Vector{Tuple{Int,Int,Vector{Float64}}},Vector{String}}}} = DataIO.load_cache(modelCachedFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29761nt DNA Sequence:\n",
       "CTTTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTT…GAGCTGCCTATATGGAAGAGCCCTAATGTGTAAAATTAA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dna = LongSequence{DNAAlphabet{4}}(sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29761nt RNA Sequence:\n",
       "CUUUCGAUCUCUUGUAGAUCUGUUCUCUAAACGAACUUU…GAGCUGCCUAUAUGGAAGAGCCCUAAUGUGUAAAAUUAA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rna = convert(LongSequence{RNAAlphabet{4}}, dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9921aa Amino Acid Sequence:\n",
       "LSISCRSVL*TNFKICVAVTRLHA*CTHAV*LITNYCR*…GLERATTFSPRPRGVRSSVQ*TMLGRAAYMEEP*CVKLX"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function padRNA(rna::LongSequence{RNAAlphabet{4}})\n",
    "    pad_length = (3 - (length(rna) % 3)) % 3\n",
    "    return rna * LongSequence{RNAAlphabet{4}}(repeat('N', pad_length))\n",
    "end\n",
    "padded = padRNA(rna)\n",
    "amn = BioSequences.translate(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesing model data\n",
    "\n",
    "At this point the object is to extract a common value for each classes to apply to equation model\n",
    "\n",
    "Each model have your own histogram generated by RRM function, extracting only de discriminative windows and regions,\n",
    "by these histogram is possible to create Binary masks to retrieve regions.\n",
    "Those regions are exclusive per class, how we know the SNPs from COVID are well distributed along the sequences, an intersection regions isn't useful.\n",
    "The example is visible by the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalHist::Union{Nothing,Vector{UInt16}} = nothing\n",
    "finalMask::BitArray = trues(maximum(x -> length(x[2][1]), model))\n",
    "\n",
    "plt = plot(title=\"Points\")\n",
    "\n",
    "for (key, (marked, _, kmers)) in model\n",
    "\n",
    "    cache_path = \"$(pwd())/.project_cache/$(key)_outmask.dat\"\n",
    "    cache::Union{Nothing,Tuple{String,Tuple{Vector{UInt16},BitArray},Vector{String}}} = DataIO.load_cache(cache_path)\n",
    "    # plot!(marked)\n",
    "\n",
    "    finalMask[1:length(marked)] = finalMask[1:length(marked)] .* marked\n",
    "    if isnothing(finalHist)\n",
    "        finalHist = cache[2][1]\n",
    "    else\n",
    "        current_hist = cache[2][1]\n",
    "        max_len = max(length(finalHist), length(current_hist))\n",
    "\n",
    "        # Create padded versions\n",
    "        padded_final = [finalHist; ones(UInt16, max_len - length(finalHist))]\n",
    "        padded_current = [current_hist; ones(UInt16, max_len - length(current_hist))]\n",
    "\n",
    "        # Element-wise multiplication\n",
    "        finalHist = padded_final .* padded_current\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "start = 0\n",
    "current = false\n",
    "\n",
    "\n",
    "plot!(finalHist)\n",
    "plot(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variants datas (change var name to explore all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29573, 29579, [NaN])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var = \"Alpha\" # Alpha, Gamma, Beta, Omicron, Delta\n",
    "cache_path = \"$(pwd())/.project_cache/$(var)_outmask.dat\"\n",
    "cache::Union{Nothing,Tuple{String,Tuple{Vector{UInt16},BitArray},Vector{String}}} = DataIO.load_cache(cache_path)\n",
    "\n",
    "# model::Union{Nothing,Dict{String,Tuple{BitArray,Vector{Tuple{Int,Int,Vector{Float64}}},Vector{String}}}}\n",
    "# Window Histogram\n",
    "wnwHist = cache[2][1]\n",
    "rrm = model[var][2][1]\n",
    "\n",
    "for (i, e, cross) in model[var][2]\n",
    "\n",
    "    if isempty(filter(ii -> cross[ii] > 0, eachindex(cross)))\n",
    "        rrm = (i, e, cross)\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "rrm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = Vector{UnitRange{Int64}}()\n",
    "for (i, bit) in enumerate(finalMask)\n",
    "    if bit && !current\n",
    "        start = i\n",
    "        current = true\n",
    "    elseif !bit && current\n",
    "        current = false\n",
    "        push!(teste, (start:i-1))\n",
    "    end\n",
    "end\n",
    "if current\n",
    "    push!(teste, (start:length(finalMask)))\n",
    "end\n",
    "\n",
    "@show teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a common value between classes\n",
    "\n",
    "How the number of window regions and kmers are exclusive and variable between the classes, we need to crete a way to evaluate them as a unique way. THe approcah here wil bbe based on probabilities and entropy value.\n",
    "\n",
    "So the steps applied will be:\n",
    "\n",
    "1. **For each Class**\n",
    "- Calculate the probability of appearence of kmers in each window region Pk = kmer_count/kmer_total;\n",
    "- Calcuate the Shannon entropy for that class based on the propabilities applied in the class regions and kmers probalities.\n",
    "\n",
    "2. **Mounting values to model an equation**\n",
    "\n",
    "- By know we will have an data distribution like: [\"class_1\":H(x),\"class_2\":H(x),...\"class_n\":H(x)]\n",
    "\n",
    "3. **Train model using hot-encoding and softmax**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract entropy values per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropyValues = Vector{Tuple{String,Float64}}()\n",
    "\n",
    "for (key, (marked, probs, kmers)) in model\n",
    "\n",
    "    entropy = EntropyUtil.shannonEntropy(probs)\n",
    "\n",
    "    push!(entropyValues, (key, entropy))\n",
    "end\n",
    "\n",
    "@show entropyValues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and predicting models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function softmax(z::AbstractMatrix)\n",
    "    # Numerically stable implementation\n",
    "    max_z = maximum(z, dims=ndims(z))\n",
    "    exp_z = exp.(z .- max_z)\n",
    "    sum_exp = sum(exp_z, dims=ndims(z))\n",
    "    return exp_z ./ sum_exp\n",
    "end\n",
    "\n",
    "function onehotencode(labels::Union{Integer,AbstractArray{<:Integer}}, n_classes::Integer)\n",
    "    # Handle scalar input\n",
    "    if labels isa Integer\n",
    "        @assert 1 <= labels <= n_classes \"Label $labels out of range [1, $n_classes]\"\n",
    "        vec = zeros(Int, n_classes)\n",
    "        vec[labels] = 1\n",
    "        return vec\n",
    "    end\n",
    "\n",
    "    # Handle array input\n",
    "    @assert all(1 .<= labels .<= n_classes) \"Labels contain out-of-range values\"\n",
    "    matrix = zeros(Int, length(labels), n_classes)\n",
    "    for (i, label) in enumerate(labels)\n",
    "        matrix[i, label] = 1\n",
    "    end\n",
    "    return matrix\n",
    "end\n",
    "\n",
    "function cross_entropy(probabilities, y_onehot)\n",
    "    -mean(sum(y_onehot .* log.(probabilities .+ eps(Float64)), dims=2))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mutable struct SoftmaxRegressor\n",
    "    weights::Matrix{Float64}\n",
    "    bias::Matrix{Float64}\n",
    "    learning_rate::Float64\n",
    "    classes::Int\n",
    "end\n",
    "\n",
    "\n",
    "function train!(model::SoftmaxRegressor, X::Matrix{Float64}, y::Vector{Int}, epochs=1000)\n",
    "    m, n = size(X)\n",
    "    y_onehot = onehotencode(y, model.classes)\n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        # Forward pass\n",
    "        logits = X * model.weights .+ model.bias\n",
    "        probabilities = softmax(logits)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = cross_entropy(probabilities, y_onehot)\n",
    "\n",
    "        # Backward pass\n",
    "        grad_logits = probabilities .- y_onehot\n",
    "        grad_logits ./= m\n",
    "\n",
    "        # Update parameters\n",
    "        model.weights .-= model.learning_rate * X' * grad_logits\n",
    "        model.bias .-= model.learning_rate * sum(grad_logits; dims=1)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0\n",
    "            @info \"Epoch $epoch - Loss: $loss\"\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model_struct = SoftmaxRegressor(\n",
    "    randn(1, 5),  # weights (input_features × num_classes)\n",
    "    zeros(1, 5),  # bias (1 × num_classes)\n",
    "    0.01,\n",
    "    5\n",
    ")\n",
    "# (\"Omicron\", 9.278283453361743)\n",
    "# (\"Beta\", 7.030505818827168)\n",
    "# (\"Gamma\", 0.0)\n",
    "# (\"Delta\", 3.458885798328118)\n",
    "# (\"Alpha\", 7.053015329074126)\n",
    "# Sample data\n",
    "X = [9.278283453361743; 7.030505818827168; 0.0; 3.458885798328118; 7.053015329074126]\n",
    "X = reshape(X, :, 1)\n",
    "# y = [\"Omicron\", \"Beta\", \"Gamma\", \"Delta\", \"Alpha\"]\n",
    "y = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Train the model\n",
    "train!(model_struct, X, y, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(model::SoftmaxRegressor, X::Union{AbstractVector{Float64},Number})\n",
    "    # Convert input to matrix format (n_samples × 1)\n",
    "    X_matrix = X isa Number ? reshape([Float64(X)], 1, 1) : reshape(X, :, 1)\n",
    "\n",
    "    # Compute logits (n_samples × n_classes)\n",
    "    logits = X_matrix * model.weights .+ model.bias\n",
    "\n",
    "    # Compute probabilities using row-wise softmax\n",
    "    probabilities = softmax(logits)\n",
    "\n",
    "    # Get predicted class indices (1-based)\n",
    "    max_indices = argmax(probabilities; dims=2)\n",
    "    classes = [idx[2] for idx in max_indices]\n",
    "\n",
    "    return classes\n",
    "end\n",
    "\n",
    "X_test = [9.5, 2.0]\n",
    "predictions = predict(model_struct, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid multi block approach\n",
    "\n",
    "Optimisers: https://fluxml.ai/Optimisers.jl/stable/api/\n",
    "\n",
    "Decision Tree: https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Decision_Tree_Regression_Julia.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct ClassificationBlockRegressor\n",
    "    class::String\n",
    "    models::Vector{Tuple{Int,Int,Vector{Float64},Any}}\n",
    "end\n",
    "\n",
    "mutable struct ModelClassBlockStruct\n",
    "    blockmodelchain::Array{ClassificationBlockRegressor}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainSigmoidReg (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "function createModelBlock(\n",
    "    blockClassName::String,\n",
    "    regionsFourierCoefs::Vector{Tuple{Int,Int,Vector{Float64}}},\n",
    "    trueSequences::Vector{String},\n",
    "    falseSequences::Vector{String})::ClassificationBlockRegressor\n",
    "\n",
    "    block_model = ClassificationBlockRegressor(blockClassName, Vector{Tuple{Int,Int,Any}}())\n",
    "\n",
    "    # para cada região criar uma sigmoid para comparação entre sinais\n",
    "    for (initIdx, endIdx, cross) in regionsFourierCoefs\n",
    "        X = Vector{Vector{Float64}}()\n",
    "        labels = Vector{Float16}()\n",
    "\n",
    "        freIndexes::Vector{Int} = filter(ii -> cross[ii] > 0, eachindex(cross))\n",
    "\n",
    "        if !isempty(freIndexes)\n",
    "\n",
    "            minFreq::Int = maximum(freIndexes)\n",
    "            maxFreq::Int = maximum(freIndexes)\n",
    "\n",
    "            #extract all the fft from each region\n",
    "            for seq in falseSequences\n",
    "                if lastindex(seq) >= endIdx\n",
    "                    num = DataIO.sequence2AminNumSerie(seq[initIdx:endIdx])\n",
    "                    dft = abs.(rfft(num)[2:end])\n",
    "\n",
    "                    push!(X, dft[minFreq:maxFreq])\n",
    "                    push!(labels, zero(Float16))\n",
    "                end\n",
    "            end\n",
    "            for seq in trueSequences\n",
    "                if lastindex(seq) >= endIdx\n",
    "                    num = DataIO.sequence2AminNumSerie(seq[initIdx:endIdx])\n",
    "                    dft = abs.(rfft(num)[2:end])\n",
    "\n",
    "                    push!(X, dft[minFreq:maxFreq])\n",
    "                    push!(labels, one(Float16))\n",
    "                end\n",
    "            end\n",
    "            # regionBlockModel = trainLinearReg(X, labels, epochs)\n",
    "            regionBlockModel = RandomForestRegressor(n_trees=length(X[1]) * 10)\n",
    "\n",
    "            X_matrix = reduce(hcat, X) |> permutedims\n",
    "\n",
    "            # Ensure y is a row vector with correct dimensions (1 x n_samples)\n",
    "            DecisionTree.fit!(regionBlockModel, X_matrix, labels)\n",
    "            push!(block_model.models, (initIdx, endIdx, cross, regionBlockModel))\n",
    "        end\n",
    "    end\n",
    "    return block_model\n",
    "end\n",
    "\n",
    "function trainLinearReg(X::Vector{Vector{Float64}}, y::Vector{Float64}, epochs)\n",
    "    X_matrix = hcat(X...)  # Convert list of vectors into a matrix\n",
    "    n_features, n_samples = size(X_matrix)  # Get the correct dimensions\n",
    "\n",
    "    # Ensure y is a row vector with correct dimensions (1 x n_samples)\n",
    "    y = reshape(y, 1, n_samples)\n",
    "\n",
    "    # μ = mean(X_matrix, dims=2)\n",
    "    # σ = std(X_matrix, dims=2)\n",
    "    # X_normalized = (X_matrix .- μ) ./ (σ .+ 1e-6)\n",
    "\n",
    "\n",
    "    model = Chain(\n",
    "        # Dense(n_features => 64, relu),\n",
    "        # Dropout(0.5),\n",
    "        Dense(n_features => 64, relu),\n",
    "        Dense(32 => 1, sigmoid))\n",
    "\n",
    "    # Função de perda (cross-entropy)\n",
    "    # loss( x, y) = Flux.Losses.mse(model(x), y)\n",
    "    loss(x, y) = Flux.binarycrossentropy(model(x), y)\n",
    "\n",
    "    opt = Optimisers.Adam()\n",
    "    state = Optimisers.setup(opt, model)\n",
    "\n",
    "    # data_loader = Flux.DataLoader((X_matrix, y), batchsize=32, shuffle=true)\n",
    "    for epoch in 1:epochs\n",
    "        # for (x_batch, y_batch) in data_loader\n",
    "        grads = Flux.gradient(model) do m\n",
    "            loss(X_matrix, y)\n",
    "        end\n",
    "\n",
    "        # Atualiza os pesos do modelo\n",
    "        Optimisers.update!(state, model, grads)\n",
    "        # @info \"Epoch $epoch\" loss = loss(X_matrix, y)\n",
    "\n",
    "    end\n",
    "    return model\n",
    "end\n",
    "\n",
    "\n",
    "function trainSigmoidReg(X::Vector{Vector{Float64}}, y::Vector{Int8}, epochs=1000)\n",
    "\n",
    "\n",
    "    # Converter X para matriz (cada coluna é uma amostra)\n",
    "    X_matrix = hcat(X...)  # Transforma lista de vetores em matriz\n",
    "    n_features = size(X_matrix, 1)\n",
    "\n",
    "    y = y'\n",
    "\n",
    "    # Definição do modelo\n",
    "    model = Chain(Dense(n_features, 1, σ), sigmoid)\n",
    "\n",
    "    # Função de perda (cross-entropy)\n",
    "    loss(model, x, y) = Flux.Losses.binarycrossentropy(model(x), y)\n",
    "\n",
    "    # Configuração do otimizador no novo estilo do Flux\n",
    "    opt = Optimisers.Adam(0.005)\n",
    "    state = Optimisers.setup(opt, model)\n",
    "\n",
    "    # Loop de treinamento\n",
    "    for epoch in 1:epochs\n",
    "        grads = Flux.gradient(model) do m\n",
    "            loss(m, X_matrix, y)\n",
    "        end\n",
    "\n",
    "        # Atualiza os pesos do modelo\n",
    "        Optimisers.update!(state, model, grads)\n",
    "\n",
    "        if epoch % 10 == 0\n",
    "            println(\"Epoch $epoch - Loss: \", loss(model, X_matrix, y))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return model\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "StackOverflowError",
     "evalue": "StackOverflowError:",
     "output_type": "error",
     "traceback": [
      "StackOverflowError:\n",
      "\n",
      "Stacktrace:\n",
      "     [1] plan_rfft(x::Vector{Float64}, region::UnitRange{Int64}; kws::@Kwargs{})\n",
      "       @ AbstractFFTs ~/.julia/packages/AbstractFFTs/4iQz5/src/definitions.jl:221--- the above 1 lines are repeated 39990 more times ---"
     ]
    }
   ],
   "source": [
    "classes = [\"Alpha\", \"Delta\", \"Beta\", \"Gamma\", \"Omicron\"]\n",
    "mdelstruct = ModelClassBlockStruct(Array{ClassificationBlockRegressor}(undef, length(classes)))\n",
    "\n",
    "@inbounds for i in eachindex(classes)\n",
    "    class::String = classes[i]\n",
    "    # para cada classe criar seu bloco de modelos sigmois para cada região\n",
    "    falseSequences = Vector{String}()\n",
    "    trueSequences = Vector{String}()\n",
    "    for key in classes\n",
    "        cache_path = \"$(pwd())/.project_cache/$(key)_outmask.dat\"\n",
    "        vardata::Union{Nothing,Tuple{String,Tuple{Vector{UInt16},BitArray},Vector{String}}} = DataIO.load_cache(cache_path)\n",
    "        sqs::Vector{String} = vardata[3]\n",
    "\n",
    "        @inbounds for i in eachindex(sqs)\n",
    "            # push!(falseSequences, DataIO.sequence2AminNumSerie(sqs[i]))\n",
    "            if key != class\n",
    "                push!(falseSequences, sqs[i])\n",
    "            else\n",
    "                push!(trueSequences, sqs[i])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    mdelstruct.blockmodelchain[i] = createModelBlock(class, model[class][2], trueSequences, falseSequences)\n",
    "end\n",
    "\n",
    "mdelstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputSequence = sequences[1]\n",
    "\n",
    "\n",
    "open(\"reportFilename.txt\", \"w\") do file\n",
    "    for block in mdelstruct.blockmodelchain\n",
    "        write(file, \"\\n\\n########### $(uppercase(block.class)) ############\")\n",
    "        for (initi, endi, cross, c_model) in block.models\n",
    "\n",
    "\n",
    "            freIndexes::Vector{Int} = filter(ii -> cross[ii] >= 0, eachindex(cross))\n",
    "            if !isempty(freIndexes)\n",
    "                minFreq::Int = maximum(freIndexes)\n",
    "                maxFreq::Int = maximum(freIndexes)\n",
    "\n",
    "                if lastindex(inputSequence) >= endi\n",
    "                    num = DataIO.sequence2AminNumSerie(inputSequence[initi:endi])\n",
    "                    dft = abs.(rfft(num)[2:end])\n",
    "\n",
    "                    norm = dft[minFreq:maxFreq]\n",
    "                    # raw_output = c_model(reshape(norm, :, 1))[1]\n",
    "                    input = reduce(hcat, [norm]) |> permutedims\n",
    "                    raw_output = predict(c_model, input)[1]\n",
    "\n",
    "                    write(file, \"\\nWindow Position( $initi - $endi ): \\nClassification: $(raw_output >= 0.5f0)\")\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "    end\n",
    "end\n",
    "\n",
    "# Classification.classifyInput(codeunits(inputSequence), model, nothing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifySequence (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function classifySequence(blockModel::ModelClassBlockStruct, inputSequence::String)\n",
    "\n",
    "    classifications = Dict{String,Float16}()\n",
    "    for block in blockModel.blockmodelchain\n",
    "\n",
    "        predictions::BitArray = []\n",
    "        for (initi, endi, cross, c_model) in block.models\n",
    "            freIndexes::Vector{Int} = filter(ii -> cross[ii] >= 0, eachindex(cross))\n",
    "            if !isempty(freIndexes)\n",
    "                minFreq::Int = maximum(freIndexes)\n",
    "                maxFreq::Int = maximum(freIndexes)\n",
    "\n",
    "                if lastindex(inputSequence) >= endi\n",
    "                    num = DataIO.sequence2AminNumSerie(inputSequence[initi:endi])\n",
    "                    dft = abs.(rfft(num)[2:end])\n",
    "                    norm = dft[minFreq:maxFreq]\n",
    "                    input = reduce(hcat, [norm]) |> permutedims\n",
    "                    raw_output = predict(c_model, input)[1]\n",
    "\n",
    "                    push!(predictions, raw_output >= 0.5f0)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        classifications[block.class] = count(i -> i, predictions) / length(predictions)\n",
    "    end\n",
    "    maxPercent = maximum(x -> x[2], classifications)\n",
    "    return findfirst(x -> x == maxPercent, classifications), maxPercent, classifications\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Omicron\", Float16(0.909), Dict{String, Float16}(\"Omicron\" => 0.909, \"Beta\" => 0.0, \"Gamma\" => 0.03226, \"Delta\" => 0.0, \"Alpha\" => 0.0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputSequence = sequences[10]\n",
    "c = classifySequence(mdelstruct, inputSequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{String}:\n",
       " \"Alpha\"\n",
       " \"Delta\"\n",
       " \"Beta\"\n",
       " \"Gamma\"\n",
       " \"Omicron\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confMatrix = Dict{String,Tuple{Int,Int}}()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Tuple{Int64, Int64}} with 5 entries:\n",
       "  \"Omicron\" => (129, 300)\n",
       "  \"Beta\"    => (270, 300)\n",
       "  \"Gamma\"   => (261, 300)\n",
       "  \"Delta\"   => (263, 300)\n",
       "  \"Alpha\"   => (246, 300)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for class in classes\n",
    "    classeqs = Vector{String}()\n",
    "    for record in open(FASTAReader, \"/home/salipe/Desktop/datasets/test_voc/test/$class.fasta\")\n",
    "        seq::String = sequence(String, record)\n",
    "        push!(classeqs, seq)\n",
    "    end\n",
    "\n",
    "    classifications = String[]\n",
    "    for seq in classeqs\n",
    "        cl, _, _ = classifySequence(mdelstruct, seq)\n",
    "        # cl, _, _ = Classification.classifyInput(codeunits(seq), model, nothing)\n",
    "        push!(classifications, cl)\n",
    "    end\n",
    "    confMatrix[class] = (count(x -> x == class, classifications), length(classifications))\n",
    "\n",
    "end\n",
    "confMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "Thrash content trying to extract some values from instecsioned windows\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = Vector{Tuple{String,Vector{Int16}}}()\n",
    "(id, inputSequence) = sequences[1]\n",
    "\n",
    "for (key, (marked, _, kmers)) in model\n",
    "    probs = Vector{Integer}()\n",
    "    inputlen = minimum(length, [inputSequence, marked])\n",
    "    limitedMark::BitArray = marked[1:inputlen]\n",
    "    start = 0\n",
    "    current = false\n",
    "\n",
    "    for (i, bit) in enumerate(limitedMark)\n",
    "        if bit && !current\n",
    "            start = i\n",
    "            current = true\n",
    "        elseif !bit && current\n",
    "            current = false\n",
    "            count = @views Classification.countPatterns(inputSequence[start:i-1], kmers)\n",
    "\n",
    "            push!(probs, convert(Int16, count))\n",
    "        end\n",
    "    end\n",
    "    if current\n",
    "        if (length([start:inputlen]) < length(kmers[1]))\n",
    "            start = start - length(kmers[1])\n",
    "        end\n",
    "\n",
    "        count = @views Classification.countPatterns(inputSequence[start:inputlen], kmers)\n",
    "        push!(probs, convert(Int16, count))\n",
    "\n",
    "\n",
    "    end\n",
    "\n",
    "    push!(chart, (key, probs))\n",
    "end\n",
    "\n",
    "\n",
    "@show chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_len = maximum(x -> length(x[2]), chart)\n",
    "x = [reshape([var[2]; zeros(Int16, max_len - length(var[2]))], (1, max_len)) for var in chart]\n",
    "\n",
    "N, L, C = 1, max_len, 5  # Batch size 1, sequence length 35, 5 classes\n",
    "logits = Array{Int16}(undef, 1, max_len, 5)\n",
    "\n",
    "for i in 1:length(x)\n",
    "    logits[:, :, i] = x[i]\n",
    "end\n",
    "\n",
    "\n",
    "y_indices = rand(1:C, N, L)  # Class indices for each position\n",
    "\n",
    "# Convert to one-hot (optional)\n",
    "y_true = Flux.onehotbatch(vec(y_indices), 1:C)  # Shape (C, N*L)\n",
    "y_true = reshape(y_true, C, N, L)  # Reshape to (N, L, C)\n",
    "y_true = permutedims(y_true, (2, 3, 1))  # Final shape (N, L, C)\n",
    "\n",
    "# Compute loss\n",
    "log_probs = Flux.logsoftmax(logits; dims=3)\n",
    "loss = -mean(y_true .* log_probs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show y_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
